{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course Project: Galaxy Classifier\n",
    "\n",
    "### Lexy Andershock, Gian Fernandez-Aleman, David Long\n",
    "\n",
    "#### Introduction\n",
    "This course project for our Intro to Machine Learning course (COSC325) utilizes a logistic regression model on a dataset of colored galaxy images (Galaxy10 SDSS). While the dataset was originally used with a CNN (a more appropriate model for image data), we wanted to experiment and see what we could do with it using one of the learning techniques we've learned in class.\n",
    "\n",
    "#### Who is this for?\n",
    "Our target audience for this project would include astronomers, physicists, space enthusiasts, space researchers, educators, and general populations with an interest in galaxy-shape indentification.\n",
    "\n",
    "#### Purpose of the Project\n",
    "The problem we are trying to solve is identifying the shapes of newly discovered galaxies. This dataset contains 10 unique categories of different shapes galaxies possess, and we want to train a model to be able to assign one of these categories to new images of galaxies it hasn't encountered yet. With new space telescopes such as JWST, we are discovering new galaxies at an accelerating rate, and being able to quickly classify them for studying would be very useful instead of manually doing it. This will make users' lives significantly better as they can have a system that handles massive amounts of galaxy photosets and produces accurate results, which is more time-efficient as opposed to manually determining the galaxy shapes.\n",
    "\n",
    "### 1. Setting Up The Environment\n",
    "We must ensure that we have imported all the appropriate libraries that we will utilize for our data manipulation and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras import utils\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# Scikit-learn libraries and routines\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Organizing Data\n",
    "We then extract our data from the dataset and organize them into two separate categories: images and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'h5py' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# To get the images and labels from file\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m h5py\u001b[38;5;241m.\u001b[39mFile(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGalaxy10.h5\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m data:\n\u001b[0;32m      3\u001b[0m     images \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      4\u001b[0m     labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mans\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'h5py' is not defined"
     ]
    }
   ],
   "source": [
    "# To get the images and labels from file\n",
    "with h5py.File('Galaxy10.h5', 'r') as data:\n",
    "    images = np.array(data['images'])\n",
    "    labels = np.array(data['ans'])\n",
    "\n",
    "# To convert the labels to categorical 10 classes\n",
    "labels = utils.to_categorical(labels, 10)\n",
    "\n",
    "# To convert to desirable type\n",
    "labels = labels.astype(np.float32)\n",
    "images = images.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Splitting Data\n",
    "Once we have the data put into separate categories, we split it into training and testing categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "train_idx, test_idx = train_test_split(np.arange(labels.shape[0]), test_size=0.1)\n",
    "train_images, train_labels, test_images, test_labels = images[train_idx], labels[train_idx], images[test_idx], labels[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Predicting\n",
    "We predict this shit fr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\longd\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Because we are dealing with image data and we are working with logistic\n",
    "# regression, a linear model, the images will have to be flattened.\n",
    "n_samples, height, width, channels = train_images.shape\n",
    "train_images_flat = train_images.reshape(n_samples, -1)\n",
    "test_images_flat = test_images.reshape(test_images.shape[0], -1)\n",
    "\n",
    "# Standardizing the data will allow logistic regression to perform better\n",
    "scaler = StandardScaler()\n",
    "train_images_scaled = scaler.fit_transform(train_images_flat)\n",
    "test_images_scaled = scaler.transform(test_images_flat)\n",
    "\n",
    "log_reg = OneVsRestClassifier(LogisticRegression(max_iter=5000, multi_class='ovr', solver='saga'))\n",
    "\n",
    "log_reg.fit(train_images_scaled, np.argmax(train_labels, axis=1))\n",
    "\n",
    "predictions = log_reg.predict(test_images_scaled)\n",
    "\n",
    "cm = confusion_matrix(np.argmax(test_labels, axis=1), predictions)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(np.argmax(test_labels, axis=1), predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
